import time  
import streamlit as st
import pandas as pd
import re
import jieba
import numpy as np
from pypinyin import lazy_pinyin, Style
from datetime import datetime
from snownlp import SnowNLP
from sklearn.feature_extraction.text import TfidfVectorizer
from scipy.sparse import hstack
import joblib
import requests

# åˆå§‹åŒ–sessionçŠ¶æ€
if 'raw_df' not in st.session_state:
    st.session_state.raw_df = None
if 'cleaned_df' not in st.session_state:
    st.session_state.cleaned_df = None
if 'predicted_df' not in st.session_state:
    st.session_state.predicted_df = None

# åŠ è½½æ¨¡å‹å’Œé¢„å¤„ç†å¯¹è±¡
if 'model' not in st.session_state:
    try:
        st.session_state.model = joblib.load('model.joblib')
        st.session_state.tfidf = joblib.load('tfidf_vectorizer.joblib')
        category_mappings = joblib.load('category_mappings.joblib')
        st.session_state.region_mapping = category_mappings['region']
        st.session_state.product_mapping = category_mappings['product']
    except Exception as e:
        st.error(f"åˆå§‹åŒ–å¤±è´¥: {str(e)}")

# é¡µé¢é…ç½®
st.set_page_config(page_title="CSVæ•°æ®æ¸…æ´—å·¥å…·", layout="wide")
st.title("è‡ªåŠ¨åŒ–æ•°æ®æ¸…æ´—ä¸æ¨èé¢„æµ‹ç³»ç»Ÿ")

# ---------------------- æ•°æ®æ¸…æ´—å‡½æ•° ----------------------
def cleaning(df):
    """æ ¸å¿ƒæ¸…æ´—é€»è¾‘"""
    progress = st.progress(0)
    status = st.status("ğŸš€ æ­£åœ¨å¤„ç†æ•°æ®...")
    
    try:
        # æ­¥éª¤1ï¼šåŸºç¡€è¿‡æ»¤
        status.write("1. è¿‡æ»¤æ±‰å­—å°‘äº5ä¸ªçš„è¯„è®º...")
        df['æ±‰å­—æ•°'] = df['è¯„è®º'].apply(lambda x: len(re.findall(r'[\u4e00-\u9fff]', str(x))))
        df = df[df['æ±‰å­—æ•°'] > 5].drop(columns=['æ±‰å­—æ•°'])
        progress.progress(16)

        # æ–°å¢æ­¥éª¤ï¼šåˆ é™¤äº§å“ä¸ºç©ºçš„æ•°æ®
        status.write("2. åˆ é™¤äº§å“ä¿¡æ¯ç¼ºå¤±çš„è¯„è®º...")
        original_count = len(df)
        df = df.dropna(subset=['äº§å“'])  # å…³é”®ä¿®æ”¹ç‚¹[1,5](@ref)
        removed_count = original_count - len(df)
        status.write(f"å·²æ¸…é™¤{removed_count}æ¡æ— äº§å“ä¿¡æ¯çš„è®°å½•")
        progress.progress(32)

        # åŸæ­¥éª¤è°ƒæ•´ä¸ºåç»­æ­¥éª¤
        status.write("3. æ£€æµ‹é‡å¤è¯„è®º...")
        df = df[~df.duplicated(subset=['è¯„è®º'], keep='first')]
        progress.progress(48)

        status.write("4. æ£€æµ‹å¥½è¯„è¿”ç°...")
        rebate_pattern = build_rebate_pattern()
        df = df[~df['è¯„è®º'].str.contains(rebate_pattern, na=False)]
        progress.progress(64)

        status.write("5. æ£€æµ‹å¯ç–‘æ°´å†›...")
        df = filter_spam_comments(df)
        progress.progress(80)

        # æœ€ç»ˆå¤„ç†
        df = df.reset_index(drop=True)
        progress.progress(100)
        status.update(label="âœ… æ¸…æ´—å®Œæˆï¼", state="complete")
        return df
    except Exception as e:
        status.update(label="âŒ å¤„ç†å‡ºé”™ï¼", state="error")
        st.error(f"é”™è¯¯è¯¦æƒ…ï¼š{str(e)}")
        return df

def build_rebate_pattern():
    """æ„å»ºè¿”ç°æ£€æµ‹æ­£åˆ™"""
    base_keywords = ['å¥½è¯„è¿”ç°', 'æ™’å›¾å¥–åŠ±', 'è¯„ä»·æœ‰ç¤¼']
    patterns = []
    for kw in base_keywords:
        full_pinyin = ''.join(lazy_pinyin(kw, style=Style.NORMAL))
        patterns.append(re.escape(full_pinyin))
        initials = ''.join([p[0] for p in lazy_pinyin(kw, style=Style.INITIALS) if p])
        if initials:
            patterns.append(re.escape(initials))
    patterns += [
        r'è¿”\s*ç°', r'è¯„.{0,3}è¿”', 
        r'åŠ \s*å¾®', r'é¢†\s*çº¢\s*åŒ…',
        r'\d+\s*å…ƒ\s*å¥–'
    ]
    return re.compile('|'.join(patterns), flags=re.IGNORECASE)

def filter_spam_comments(df):
    """æ°´å†›æ£€æµ‹ç®—æ³•"""
    try:
        df['æ—¥æœŸ'] = pd.to_datetime(df['æ—¥æœŸ'])
        df_sorted = df.sort_values(['æ˜µç§°', 'åœ°åŒº', 'æ—¥æœŸ'])
        grouped = df_sorted.groupby(['æ˜µç§°', 'åœ°åŒº'])
        df_sorted['time_diff'] = grouped['æ—¥æœŸ'].diff().dt.total_seconds().abs()
        df_sorted['is_spam'] = (df_sorted['time_diff'] <= 300) & (df_sorted['time_diff'] > 0)
        return df_sorted[~df_sorted['is_spam']].drop(columns=['time_diff', 'is_spam'])
    except KeyError:
        return df

# ---------------------- é¢„æµ‹ç›¸å…³å‡½æ•° ----------------------
def extract_keywords(text, n=5):
    """æå–å…³é”®è¯"""
    words = [word for word in jieba.cut(str(text)) if len(word) > 1]
    return ' '.join(words[:n])

def calculate_scores(row):
    """è®¡ç®—ç‰¹å¾åˆ†æ•°"""
    try:
        text = str(row['è¯„è®º'])
        sentiment = SnowNLP(text).sentiments
        authenticity = min(len(text)/100, 1)
        relevance = len(str(row.get('å…³é”®è¯', '')).split())/10
        return pd.Series([sentiment, authenticity, relevance])
    except:
        return pd.Series([0.5, 0.5, 0.5])
    

# ----------------------  DeepSeek åˆ†ææ¨¡å— ----------------------
def generate_analysis_prompt(product_name, comments, scores):
    """æ„å»ºåˆ†ææç¤ºè¯æ¨¡æ¿"""
    return f"""è¯·æ ¹æ®ç”µå•†è¯„è®ºæ•°æ®ç”Ÿæˆäº§å“åˆ†ææŠ¥å‘Šï¼Œè¦æ±‚ï¼š
1. äº§å“åç§°ï¼š{product_name}
2. åŸºäºä»¥ä¸‹{len(comments)}æ¡çœŸå®è¯„è®ºï¼ˆè¯„åˆ†åˆ†å¸ƒï¼š{scores}ï¼‰ï¼š
{comments[:5]}...ï¼ˆæ˜¾ç¤ºå‰5æ¡ç¤ºä¾‹ï¼‰
3. è¾“å‡ºç»“æ„ï¼š
ã€äº§å“æ€»ç»“ã€‘ç”¨50å­—æ¦‚æ‹¬æ•´ä½“è¯„ä»·
ã€æ¨èæŒ‡æ•°ã€‘æ ¹æ®è¯„åˆ†åˆ†å¸ƒç»™å‡º1-10åˆ†
ã€ä¸»è¦ä¼˜ç‚¹ã€‘åˆ—å‡º3-5ä¸ªæ ¸å¿ƒä¼˜åŠ¿ï¼Œå¸¦å…·ä½“ä¾‹å­
ã€ä¸»è¦ç¼ºç‚¹ã€‘åˆ—å‡º3-5ä¸ªå…³é”®ä¸è¶³ï¼Œå¸¦å…·ä½“ä¾‹å­
ã€è´­ä¹°å»ºè®®ã€‘ç»™å‡ºæ˜¯å¦æ¨èçš„ç»“è®ºåŠåŸå› 
è¯·ç”¨markdownæ ¼å¼è¾“å‡ºï¼Œé¿å…ä¸“ä¸šæœ¯è¯­ï¼Œä¿æŒå£è¯­åŒ–"""

def call_deepseek_api(prompt):
    """è°ƒç”¨DeepSeek API"""
    api_key = st.secrets["DEEPSEEK_API_KEY"]  # æ­£å¼ä½¿ç”¨è¯·æ”¹ç”¨ secrets
    headers = {
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json"
    }
    data = {
        "model": "deepseek-chat",
        "messages": [{"role": "user", "content": prompt}],
        "temperature": 0.2,
        "max_tokens": 2000
    }
    
    try:
        response = requests.post("https://api.deepseek.com/v1/chat/completions", 
                                headers=headers, json=data)
        if response.status_code == 200:
            return response.json()['choices'][0]['message']['content']
        return f"APIé”™è¯¯ï¼š{response.text}"
    except Exception as e:
        return f"è¯·æ±‚å¤±è´¥ï¼š{str(e)}"

def analyze_products(df):
    """æ‰§è¡Œäº§å“åˆ†æä¸»é€»è¾‘"""
    analysis_results = {}
    start_time = time.time()  # è®°å½•å¼€å§‹æ—¶é—´
    
    with st.status("ğŸ” æ·±åº¦åˆ†æè¿›è¡Œä¸­...", expanded=True) as status:
        # æŒ‰äº§å“åˆ†ç»„åˆ†æ
        for product, group in df.groupby('äº§å“'):
            status.write(f"æ­£åœ¨åˆ†æï¼š{product}...")
            
            # å‡†å¤‡æ•°æ®
            comments = group['è¯„è®º'].tolist()
            scores = group['ç³»ç»Ÿæ¨èæŒ‡æ•°'].value_counts().to_dict()
            
            # ç”Ÿæˆæç¤ºè¯
            prompt = generate_analysis_prompt(
                product_name=product,
                comments=comments,
                scores=scores
            )
            
            # è°ƒç”¨API
            analysis_result = call_deepseek_api(prompt)
            analysis_results[product] = analysis_result
            
            time.sleep(0.08)
            
            
        duration = time.time() - start_time  # è®¡ç®—è€—æ—¶
        status.update(label=f"âœ… åˆ†æå®Œæˆï¼æ€»è€—æ—¶ {duration:.2f} ç§’", state="complete")  # æ˜¾ç¤ºè€—æ—¶
    
    return analysis_results



# ---------------------- ç•Œé¢å¸ƒå±€ ----------------------
# æ–‡ä»¶ä¸Šä¼ æ¨¡å—
uploaded_file = st.file_uploader("ä¸Šä¼ CSVæ–‡ä»¶", type=["csv"], 
                               help="æ”¯æŒUTF-8ç¼–ç æ–‡ä»¶ï¼Œæœ€å¤§100MB")

if uploaded_file and st.session_state.raw_df is None:
    st.session_state.raw_df = pd.read_csv(uploaded_file)

# æ˜¾ç¤ºåŸå§‹æ•°æ®
if st.session_state.raw_df is not None:
    with st.expander("ğŸ“‚ æ°¸ä¹…æŸ¥çœ‹åŸå§‹æ•°æ®", expanded=True):
        st.write(f"åŸå§‹è®°å½•æ•°ï¼š{len(st.session_state.raw_df)}")
        st.dataframe(
            st.session_state.raw_df,
            use_container_width=True,
            height=500  # è®¾ç½®å›ºå®šé«˜åº¦å¯ç”¨æ»šåŠ¨æ¡
        )
        
# æ•°æ®æ¸…æ´—æ¨¡å—
if st.session_state.raw_df is not None:
    st.divider()
    st.subheader("æ•°æ®æ¸…æ´—æ¨¡å—")
    
    col1, col2 = st.columns([1,3])
    with col1:
        if st.button("ğŸš€ å¼€å§‹æ¸…æ´—", help="ç‚¹å‡»å¼€å§‹ç‹¬ç«‹æ¸…æ´—æµç¨‹", use_container_width=True):
            with st.spinner('æ­£åœ¨å¤„ç†æ•°æ®...'):
                start_time = time.time()
                st.session_state.cleaned_df = cleaning(st.session_state.raw_df.copy())
                st.session_state.processing_time = time.time() - start_time

    if st.session_state.cleaned_df is not None:
        with col2:
            if st.button("ğŸ” æŸ¥çœ‹æ¸…æ´—ç»“æœ", help="ç‹¬ç«‹æŸ¥çœ‹æ¸…æ´—æ•°æ®", use_container_width=True):
                with st.expander("âœ¨ æ¸…æ´—åæ•°æ®è¯¦æƒ…", expanded=True):
                    st.write(f"æ¸…æ´—åè®°å½•æ•°ï¼š{len(st.session_state.cleaned_df)}")  # æ–°å¢æ•°é‡æ˜¾ç¤º
                    st.dataframe(
                        st.session_state.cleaned_df[['æ˜µç§°','æ—¥æœŸ','åœ°åŒº','äº§å“', 'è¯„åˆ†','è¯„è®º']],
                        use_container_width=True,
                        height=500  # è®¾ç½®æ»šåŠ¨æ¡
                    )

# é¢„æµ‹æ¨¡å—
if st.session_state.cleaned_df is not None:
    st.divider()
    st.subheader("é¢„æµ‹æ¨¡å—")
    
    if st.button("ğŸ”® é¢„æµ‹æ¨èæŒ‡æ•°", help="ç‚¹å‡»è¿›è¡Œæ¨èæŒ‡æ•°é¢„æµ‹", use_container_width=True):
        if 'model' not in st.session_state:
            st.error("æ¨¡å‹æœªåŠ è½½ï¼Œæ— æ³•è¿›è¡Œé¢„æµ‹ï¼")
            st.stop()
        
        cleaned_df = st.session_state.cleaned_df.copy()
        
        with st.status("ğŸ§  æ­£åœ¨ç”Ÿæˆé¢„æµ‹...", expanded=True) as status:
            try:
                # ç‰¹å¾å·¥ç¨‹
                status.write("1. æå–å…³é”®è¯...")
                cleaned_df['å…³é”®è¯'] = cleaned_df['è¯„è®º'].apply(lambda x: extract_keywords(x, n=5))
                
                status.write("2. è®¡ç®—æƒ…æ„Ÿç‰¹å¾...")
                scores = cleaned_df.apply(calculate_scores, axis=1)
                cleaned_df[['æƒ…æ„Ÿåº¦', 'çœŸå®æ€§', 'å‚è€ƒåº¦']] = scores
                
                # TF-IDFè½¬æ¢
                status.write("3. æ–‡æœ¬ç‰¹å¾è½¬æ¢...")
                keywords_tfidf = st.session_state.tfidf.transform(cleaned_df['å…³é”®è¯'])
                
                # æ„å»ºç‰¹å¾çŸ©é˜µ
                status.write("4. åˆå¹¶ç‰¹å¾...")
                numeric_features = cleaned_df[['æƒ…æ„Ÿåº¦', 'çœŸå®æ€§', 'å‚è€ƒåº¦']].values
                features = hstack([keywords_tfidf, numeric_features])
                
                # åˆ†ç±»ç¼–ç 
                status.write("5. å¤„ç†åˆ†ç±»ç‰¹å¾...")
                cleaned_df['åœ°åŒº_ç¼–ç '] = pd.Categorical(
                    cleaned_df['åœ°åŒº'], 
                    categories=st.session_state.region_mapping
                ).codes
                cleaned_df['äº§å“_ç¼–ç '] = pd.Categorical(
                    cleaned_df['äº§å“'],
                    categories=st.session_state.product_mapping
                ).codes
                final_features = hstack([features, cleaned_df[['åœ°åŒº_ç¼–ç ', 'äº§å“_ç¼–ç ']].values])
                
                # é¢„æµ‹
                status.write("6. è¿›è¡Œæ¨¡å‹é¢„æµ‹...")
                predicted_scores = st.session_state.model.predict(final_features)
                cleaned_df['ç³»ç»Ÿæ¨èæŒ‡æ•°'] = np.round(predicted_scores).clip(1, 10).astype(int)
                
                # ä¿å­˜ç»“æœ
                st.session_state.predicted_df = cleaned_df[['äº§å“', 'è¯„è®º', 'ç³»ç»Ÿæ¨èæŒ‡æ•°']]
                status.update(label="âœ… é¢„æµ‹å®Œæˆï¼", state="complete")
                
            except Exception as e:
                status.update(label="âŒ é¢„æµ‹å‡ºé”™ï¼", state="error")
                st.error(f"é”™è¯¯è¯¦æƒ…ï¼š{str(e)}")
                st.stop()

        # æ˜¾ç¤ºé¢„æµ‹ç»“æœ
        if st.session_state.predicted_df is not None:
            st.success("é¢„æµ‹ç»“æœï¼š")
            st.dataframe(
                st.session_state.predicted_df,
                use_container_width=True,
                height=600,  # è®¾ç½®å›ºå®šé«˜åº¦å¯ç”¨æ»šåŠ¨æ¡
                hide_index=True  # å¯é€‰ï¼šéšè—é»˜è®¤ç´¢å¼•
            )
    
            # æ·»åŠ æ•°æ®ç»Ÿè®¡ä¿¡æ¯
            st.caption(f"æ€»è®°å½•æ•°ï¼š{len(st.session_state.predicted_df)} æ¡")
    
            # ä¸‹è½½é¢„æµ‹ç»“æœï¼ˆä¿æŒåŸä»£ç ä¸å˜ï¼‰
            csv = st.session_state.predicted_df.to_csv(index=False).encode('utf-8')
            st.download_button(
                label="â¬‡ï¸ ä¸‹è½½é¢„æµ‹ç»“æœ",
                data=csv,
                file_name='predicted_scores.csv',
                mime='text/csv',
                key='prediction_download'
            )
            
# ---------------------- åœ¨é¢„æµ‹æ¨¡å—åæ·»åŠ åˆ†ææ¨¡å— ----------------------
if st.session_state.predicted_df is not None:
    st.divider()
    st.subheader("æ·±åº¦åˆ†ææ¨¡å—")
    
    if st.button("ğŸ“Š ç”Ÿæˆäº§å“åˆ†ææŠ¥å‘Š", type="primary"):
        # æ‰§è¡Œåˆ†æ
        analysis_results = analyze_products(st.session_state.predicted_df)
        
        # å±•ç¤ºç»“æœ
        for product, report in analysis_results.items():
            with st.expander(f"**{product}** å®Œæ•´åˆ†ææŠ¥å‘Š", expanded=False):
                st.markdown(report)
                
            # æ·»åŠ ä¸‹è½½æŒ‰é’®
            st.download_button(
                label=f"â¬‡ï¸ ä¸‹è½½ {product} æŠ¥å‘Š",
                data=report,
                file_name=f"{product}_analysis.md",
                mime="text/markdown"
            )
