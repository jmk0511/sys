import time  
import streamlit as st
import pandas as pd
import re
import jieba
import numpy as np
from pypinyin import lazy_pinyin, Style
from datetime import datetime
from snownlp import SnowNLP
from sklearn.feature_extraction.text import TfidfVectorizer
from scipy.sparse import hstack
import joblib
import requests

# ---------------------- åˆå§‹åŒ–å…¨å±€çŠ¶æ€ ----------------------
if 'sys' not in st.session_state:
    st.session_state.sys = {
        # æ•°æ®å­˜å‚¨
        'raw_df': None,
        'cleaned_df': None,
        'predicted_df': None,
        'analysis': {},
        
        # ç•Œé¢çŠ¶æ€
        'show_raw': True,
        'show_cleaned': False,
        'show_predicted': False,
        'show_analysis': {},
        
        # æ¨¡å‹ç›¸å…³
        'model': None,
        'tfidf': None,
        'region_map': None,
        'product_map': None
    }

# ---------------------- æ ¸å¿ƒåŠŸèƒ½å‡½æ•° ----------------------
def cleaning(df):
    """æ•°æ®æ¸…æ´—æ ¸å¿ƒé€»è¾‘"""
    progress = st.progress(0)
    status = st.status("ğŸš€ æ­£åœ¨å¤„ç†æ•°æ®...")
    
    try:
        # æ­¥éª¤1ï¼šåŸºç¡€è¿‡æ»¤
        status.write("1. è¿‡æ»¤æ±‰å­—å°‘äº5ä¸ªçš„è¯„è®º...")
        df['æ±‰å­—æ•°'] = df['è¯„è®º'].apply(lambda x: len(re.findall(r'[\u4e00-\u9fff]', str(x))))
        df = df[df['æ±‰å­—æ•°'] > 5].drop(columns=['æ±‰å­—æ•°'])
        progress.progress(16)

        # æ­¥éª¤2ï¼šåˆ é™¤äº§å“ä¸ºç©ºæ•°æ®
        status.write("2. åˆ é™¤äº§å“ä¿¡æ¯ç¼ºå¤±çš„è¯„è®º...")
        original_count = len(df)
        df = df.dropna(subset=['äº§å“'])
        removed_count = original_count - len(df)
        status.write(f"å·²æ¸…é™¤{removed_count}æ¡æ— äº§å“ä¿¡æ¯çš„è®°å½•")
        progress.progress(32)

        # æ­¥éª¤3ï¼šé‡å¤è¯„è®ºè¿‡æ»¤
        status.write("3. æ£€æµ‹é‡å¤è¯„è®º...")
        df = df[~df.duplicated(subset=['è¯„è®º'], keep='first')]
        progress.progress(48)

        # æ­¥éª¤4ï¼šè¿”ç°æ£€æµ‹
        status.write("4. æ£€æµ‹å¥½è¯„è¿”ç°...")
        rebate_pattern = build_rebate_pattern()
        df = df[~df['è¯„è®º'].str.contains(rebate_pattern, na=False)]
        progress.progress(64)

        # æ­¥éª¤5ï¼šæ°´å†›æ£€æµ‹
        status.write("5. æ£€æµ‹å¯ç–‘æ°´å†›...")
        df = filter_spam_comments(df)
        progress.progress(80)

        # æœ€ç»ˆå¤„ç†
        df = df.reset_index(drop=True)
        progress.progress(100)
        status.update(label="âœ… æ¸…æ´—å®Œæˆï¼", state="complete")
        return df
        
    except Exception as e:
        status.update(label="âŒ å¤„ç†å‡ºé”™ï¼", state="error")
        st.error(f"é”™è¯¯è¯¦æƒ…ï¼š{str(e)}")
        return df

def build_rebate_pattern():
    """æ„å»ºè¿”ç°æ­£åˆ™è¡¨è¾¾å¼"""
    base_keywords = ['å¥½è¯„è¿”ç°', 'æ™’å›¾å¥–åŠ±', 'è¯„ä»·æœ‰ç¤¼']
    patterns = []
    for kw in base_keywords:
        full_pinyin = ''.join(lazy_pinyin(kw, style=Style.NORMAL))
        patterns.append(re.escape(full_pinyin))
        initials = ''.join([p[0] for p in lazy_pinyin(kw, style=Style.INITIALS) if p])
        if initials:
            patterns.append(re.escape(initials))
    patterns += [
        r'è¿”\s*ç°', r'è¯„.{0,3}è¿”', 
        r'åŠ \s*å¾®', r'é¢†\s*çº¢\s*åŒ…',
        r'\d+\s*å…ƒ\s*å¥–'
    ]
    return re.compile('|'.join(patterns), flags=re.IGNORECASE)

def filter_spam_comments(df):
    """æ°´å†›æ£€æµ‹ç®—æ³•"""
    try:
        df['æ—¥æœŸ'] = pd.to_datetime(df['æ—¥æœŸ'])
        df_sorted = df.sort_values(['æ˜µç§°', 'åœ°åŒº', 'æ—¥æœŸ'])
        grouped = df_sorted.groupby(['æ˜µç§°', 'åœ°åŒº'])
        df_sorted['time_diff'] = grouped['æ—¥æœŸ'].diff().dt.total_seconds().abs()
        df_sorted['is_spam'] = (df_sorted['time_diff'] <= 300) & (df_sorted['time_diff'] > 0)
        return df_sorted[~df_sorted['is_spam']].drop(columns=['time_diff', 'is_spam'])
    except KeyError:
        return df

def predict_recommendation():
    """æ‰§è¡Œé¢„æµ‹æµç¨‹"""
    with st.status("ğŸ§  æ­£åœ¨ç”Ÿæˆé¢„æµ‹...", expanded=True) as status:
        try:
            cleaned_df = st.session_state.sys['cleaned_df'].copy()
            
            # ç‰¹å¾å·¥ç¨‹
            status.write("1. æå–å…³é”®è¯...")
            cleaned_df['å…³é”®è¯'] = cleaned_df['è¯„è®º'].apply(lambda x: extract_keywords(x, n=5))
            
            status.write("2. è®¡ç®—æƒ…æ„Ÿç‰¹å¾...")
            scores = cleaned_df.apply(calculate_scores, axis=1)
            cleaned_df[['æƒ…æ„Ÿåº¦', 'çœŸå®æ€§', 'å‚è€ƒåº¦']] = scores
            
            # TF-IDFè½¬æ¢
            status.write("3. æ–‡æœ¬ç‰¹å¾è½¬æ¢...")
            keywords_tfidf = st.session_state.sys['tfidf'].transform(cleaned_df['å…³é”®è¯'])
            
            # æ„å»ºç‰¹å¾çŸ©é˜µ
            status.write("4. åˆå¹¶ç‰¹å¾...")
            numeric_features = cleaned_df[['æƒ…æ„Ÿåº¦', 'çœŸå®æ€§', 'å‚è€ƒåº¦']].values
            features = hstack([keywords_tfidf, numeric_features])
            
            # åˆ†ç±»ç¼–ç 
            status.write("5. å¤„ç†åˆ†ç±»ç‰¹å¾...")
            cleaned_df['åœ°åŒº_ç¼–ç '] = pd.Categorical(
                cleaned_df['åœ°åŒº'], 
                categories=st.session_state.sys['region_map']
            ).codes
            cleaned_df['äº§å“_ç¼–ç '] = pd.Categorical(
                cleaned_df['äº§å“'],
                categories=st.session_state.sys['product_map']
            ).codes
            final_features = hstack([features, cleaned_df[['åœ°åŒº_ç¼–ç ', 'äº§å“_ç¼–ç ']].values])
            
            # é¢„æµ‹
            status.write("6. è¿›è¡Œæ¨¡å‹é¢„æµ‹...")
            predicted_scores = st.session_state.sys['model'].predict(final_features)
            cleaned_df['ç³»ç»Ÿæ¨èæŒ‡æ•°'] = np.round(predicted_scores).clip(1, 10).astype(int)
            
            # ä¿å­˜ç»“æœ
            st.session_state.sys['predicted_df'] = cleaned_df[['äº§å“', 'è¯„è®º', 'ç³»ç»Ÿæ¨èæŒ‡æ•°']]
            status.update(label="âœ… é¢„æµ‹å®Œæˆï¼", state="complete")
            st.session_state.sys['show_predicted'] = True
            
        except Exception as e:
            status.update(label="âŒ é¢„æµ‹å‡ºé”™ï¼", state="error")
            st.error(f"é”™è¯¯è¯¦æƒ…ï¼š{str(e)}")
            st.stop()

def analyze_products():
    """æ‰§è¡Œæ·±åº¦åˆ†æ"""
    analysis_results = {}
    start_time = time.time()
    
    with st.status("ğŸ” æ·±åº¦åˆ†æè¿›è¡Œä¸­...", expanded=True) as status:
        df = st.session_state.sys['predicted_df']
        for product, group in df.groupby('äº§å“'):
            status.write(f"æ­£åœ¨åˆ†æï¼š{product}...")
            
            comments = group['è¯„è®º'].tolist()
            scores = group['ç³»ç»Ÿæ¨èæŒ‡æ•°'].value_counts().to_dict()
            
            prompt = f"""è¯·æ ¹æ®ç”µå•†è¯„è®ºæ•°æ®ç”Ÿæˆåˆ†ææŠ¥å‘Šï¼Œè¦æ±‚ï¼š
1. äº§å“åç§°ï¼š{product}
2. åŸºäº{len(comments)}æ¡è¯„è®ºï¼ˆè¯„åˆ†åˆ†å¸ƒï¼š{scores}ï¼‰
3. è¾“å‡ºç»“æ„ï¼š
ã€æ€»ç»“ã€‘50å­—æ¦‚æ‹¬
ã€æ¨èæŒ‡æ•°ã€‘1-10åˆ†
ã€ä¼˜ç‚¹ã€‘3-5ä¸ªå¸¦ä¾‹å­
ã€ç¼ºç‚¹ã€‘3-5ä¸ªå¸¦ä¾‹å­
ã€å»ºè®®ã€‘æ˜¯å¦æ¨èåŠåŸå› 
ç”¨markdownæ ¼å¼ï¼Œå£è¯­åŒ–"""
            
            result = call_deepseek_api(prompt)
            analysis_results[product] = result
            time.sleep(0.1)
            
        duration = time.time() - start_time
        status.update(label=f"âœ… åˆ†æå®Œæˆï¼è€—æ—¶ {duration:.1f}ç§’", state="complete")
    
    st.session_state.sys['analysis'] = analysis_results
    for product in analysis_results:
        st.session_state.sys['show_analysis'][product] = True

def call_deepseek_api(prompt):
    """è°ƒç”¨DeepSeek API"""
    try:
        headers = {
            "Authorization": f"Bearer {st.secrets['DEEPSEEK_API_KEY']}",
            "Content-Type": "application/json"
        }
        data = {
            "model": "deepseek-chat",
            "messages": [{"role": "user", "content": prompt}],
            "temperature": 0.2,
            "max_tokens": 2000
        }
        response = requests.post("https://api.deepseek.com/v1/chat/completions", 
                               headers=headers, json=data, timeout=30)
        return response.json()['choices'][0]['message']['content']
    except Exception as e:
        return f"APIè°ƒç”¨å¤±è´¥ï¼š{str(e)}"

# ---------------------- ç•Œé¢ç»„ä»¶ ----------------------
@st.experimental_fragment
def file_uploader():
    """æ–‡ä»¶ä¸Šä¼ ç»„ä»¶"""
    uploaded_file = st.file_uploader("ä¸Šä¼ CSVæ–‡ä»¶", type=["csv"], 
                                   help="æ”¯æŒUTF-8ç¼–ç ï¼Œæœ€å¤§100MB")
    if uploaded_file and not st.session_state.sys['raw_df']:
        st.session_state.sys['raw_df'] = pd.read_csv(uploaded_file)
        st.session_state.sys['show_raw'] = True

@st.experimental_fragment
def raw_data_viewer():
    """åŸå§‹æ•°æ®æŸ¥çœ‹å™¨"""
    if st.session_state.sys['show_raw'] and st.session_state.sys['raw_df'] is not None:
        with st.expander("ğŸ“‚ åŸå§‹æ•°æ®", expanded=True):
            st.write(f"æ€»è®°å½•æ•°ï¼š{len(st.session_state.sys['raw_df'])}")
            st.dataframe(
                st.session_state.sys['raw_df'],
                height=500,
                use_container_width=True
            )
            if st.button("âŒ å…³é—­åŸå§‹æ•°æ®", key="close_raw"):
                st.session_state.sys['show_raw'] = False

@st.experimental_fragment
def cleaning_controller():
    """æ•°æ®æ¸…æ´—æ§åˆ¶å™¨"""
    if st.session_state.sys['raw_df'] is not None:
        st.divider()
        st.subheader("æ•°æ®æ¸…æ´—")
        col1, col2 = st.columns([1, 3])
        
        with col1:
            if st.button("ğŸš€ å¼€å§‹æ¸…æ´—", help="å¯åŠ¨æ¸…æ´—æµç¨‹", use_container_width=True):
                with st.spinner('å¤„ç†ä¸­...'):
                    start = time.time()
                    st.session_state.sys['cleaned_df'] = cleaning(
                        st.session_state.sys['raw_df'].copy()
                    )
                    st.session_state.sys['show_cleaned'] = True
                    st.toast(f"æ¸…æ´—å®Œæˆï¼Œè€—æ—¶{time.time()-start:.1f}ç§’")

        with col2:
            if st.session_state.sys['show_cleaned']:
                with st.expander("âœ¨ æ¸…æ´—ç»“æœ", expanded=True):
                    st.write(f"æœ‰æ•ˆè®°å½•ï¼š{len(st.session_state.sys['cleaned_df'])}æ¡")
                    st.dataframe(
                        st.session_state.sys['cleaned_df'][
                            ['æ˜µç§°','æ—¥æœŸ','åœ°åŒº','äº§å“','è¯„åˆ†','è¯„è®º']
                        ],
                        height=500,
                        use_container_width=True
                    )
                    if st.button("âŒ å…³é—­æ¸…æ´—ç»“æœ", key="close_clean"):
                        st.session_state.sys['show_cleaned'] = False

@st.experimental_fragment
def prediction_viewer():
    """é¢„æµ‹ç»“æœæŸ¥çœ‹å™¨"""
    if st.session_state.sys['cleaned_df'] is not None:
        st.divider()
        st.subheader("é¢„æµ‹åˆ†æ")
        
        if st.button("ğŸ”® ç”Ÿæˆæ¨èæŒ‡æ•°", help="å¯åŠ¨é¢„æµ‹æ¨¡å‹", use_container_width=True):
            predict_recommendation()
        
        if st.session_state.sys['show_predicted']:
            st.success("é¢„æµ‹ç»“æœ")
            st.dataframe(
                st.session_state.sys['predicted_df'],
                height=600,
                use_container_width=True,
                hide_index=True
            )
            st.caption(f"æ€»è®°å½•æ•°ï¼š{len(st.session_state.sys['predicted_df'])}æ¡")
            
            csv_data = st.session_state.sys['predicted_df'].to_csv(index=False).encode('utf-8')
            st.download_button(
                label="â¬‡ï¸ ä¸‹è½½é¢„æµ‹ç»“æœ",
                data=csv_data,
                file_name='predictions.csv',
                mime='text/csv',
                key='dl_pred'
            )
            if st.button("âŒ å…³é—­é¢„æµ‹ç»“æœ", key="close_pred"):
                st.session_state.sys['show_predicted'] = False

@st.experimental_fragment
def analysis_reporter():
    """åˆ†ææŠ¥å‘Šç”Ÿæˆå™¨"""
    if st.session_state.sys['predicted_df'] is not None:
        st.divider()
        st.subheader("æ·±åº¦åˆ†æ")
        
        if st.button("ğŸ“Š ç”Ÿæˆåˆ†ææŠ¥å‘Š", type="primary"):
            analyze_products()
        
        for product in st.session_state.sys['analysis']:
            if st.session_state.sys['show_analysis'].get(product, False):
                with st.expander(f"ğŸ“ˆ {product} åˆ†ææŠ¥å‘Š", expanded=True):
                    st.markdown(st.session_state.sys['analysis'][product])
                    st.download_button(
                        label=f"â¬‡ï¸ ä¸‹è½½{product}æŠ¥å‘Š",
                        data=st.session_state.sys['analysis'][product],
                        file_name=f"{product}_analysis.md",
                        mime="text/markdown",
                        key=f"dl_{product}"
                    )
                    if st.button(f"âŒ å…³é—­{product}æŠ¥å‘Š", key=f"close_{product}"):
                        st.session_state.sys['show_analysis'][product] = False

# ---------------------- ä¸»ç¨‹åºå…¥å£ ----------------------
def main():
    # åˆå§‹åŒ–æ¨¡å‹
    if not st.session_state.sys['model']:
        try:
            st.session_state.sys.update({
                'model': joblib.load('model.joblib'),
                'tfidf': joblib.load('tfidf_vectorizer.joblib'),
                'region_map': joblib.load('category_mappings.joblib')['region'],
                'product_map': joblib.load('category_mappings.joblib')['product']
            })
        except Exception as e:
            st.error(f"æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}")
            return

    # é¡µé¢é…ç½®
    st.set_page_config(page_title="æ™ºèƒ½æ•°æ®å·¥å‚", layout="wide")
    st.title("ğŸ“Š ç”µå•†è¯„è®ºåˆ†æç³»ç»Ÿ")
    
    # åŠŸèƒ½ç»„ä»¶
    file_uploader()
    raw_data_viewer()
    cleaning_controller()
    prediction_viewer()
    analysis_reporter()
    
    # è‡ªåŠ¨åˆ·æ–°
    if any([st.session_state.sys['show_cleaned'], 
           st.session_state.sys['show_predicted'],
           any(st.session_state.sys['show_analysis'].values())]):
        st.rerun()

if __name__ == "__main__":
    main()