import time  
import streamlit as st
import pandas as pd
import re
import jieba
import numpy as np
from pypinyin import lazy_pinyin, Style
from datetime import datetime
from snownlp import SnowNLP
from sklearn.feature_extraction.text import TfidfVectorizer
from scipy.sparse import hstack
import joblib
import requests
import os
import io
import zipfile
import sqlite3
import bcrypt
from pathlib import Path

# ====================== ç”¨æˆ·è®¤è¯æ¨¡å— ======================
def init_auth_db():
    """åˆå§‹åŒ–æ•°æ®åº“è¿æ¥ï¼ˆæ–°å¢é¢„æµ‹è®°å½•è¡¨å’Œè¯„è®ºè¡¨ï¼‰[6,8](@ref)"""
    conn = sqlite3.connect('user_auth.db', check_same_thread=False)
    cursor = conn.cursor()
    
    # ç”¨æˆ·è¡¨
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS users (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            username TEXT UNIQUE NOT NULL,
            password_hash TEXT NOT NULL,
            created_at DATETIME DEFAULT CURRENT_TIMESTAMP
        )
    ''')
    
    # ä¸»æ•°æ®è¡¨
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS user_data (
            data_id INTEGER PRIMARY KEY AUTOINCREMENT,
            user_id INTEGER NOT NULL,
            upload_time DATETIME DEFAULT CURRENT_TIMESTAMP,
            raw_data BLOB,
            cleaned_data BLOB,
            FOREIGN KEY(user_id) REFERENCES users(id)
        )
    ''')
    
    # æ–°å¢é¢„æµ‹è®°å½•è¡¨[6](@ref)
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS predictions (
            prediction_id INTEGER PRIMARY KEY AUTOINCREMENT,
            user_id INTEGER NOT NULL,
            prediction_time DATETIME DEFAULT CURRENT_TIMESTAMP,
            model_version TEXT DEFAULT 'v1.2',
            result_data BLOB,
            FOREIGN KEY(user_id) REFERENCES users(id)
        )
    ''')
    
    # æ–°å¢è¯„è®ºå…³è”è¡¨[8](@ref)
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS comments (
            comment_id INTEGER PRIMARY KEY AUTOINCREMENT,
            prediction_id INTEGER NOT NULL,
            content TEXT,
            score INTEGER,
            FOREIGN KEY(prediction_id) REFERENCES predictions(prediction_id)
        )
    ''')
    
    conn.commit()
    return conn

@st.cache_resource
def get_auth_db():
    """è·å–æŒä¹…åŒ–æ•°æ®åº“è¿æ¥ï¼ˆå¯ç”¨WALæ¨¡å¼æå‡æ€§èƒ½ï¼‰[6,8](@ref)"""
    conn = init_auth_db()
    conn.execute("PRAGMA journal_mode=WAL")
    return conn

def register_user(username, password):
    """ç”¨æˆ·æ³¨å†Œé€»è¾‘"""
    conn = get_auth_db()
    try:
        if conn.execute('SELECT username FROM users WHERE username = ?', (username,)).fetchone():
            return False, "ç”¨æˆ·åå·²å­˜åœ¨"
        
        hashed_pw = bcrypt.hashpw(password.encode('utf-8'), bcrypt.gensalt())
        conn.execute('INSERT INTO users (username, password_hash) VALUES (?, ?)', 
                    (username, hashed_pw.decode('utf-8')))
        conn.commit()
        return True, "æ³¨å†ŒæˆåŠŸ"
    except Exception as e:
        return False, str(e)

def verify_login(username, password):
    """ç”¨æˆ·ç™»å½•éªŒè¯"""
    conn = get_auth_db()
    user = conn.execute('''
        SELECT id, password_hash FROM users WHERE username = ?
    ''', (username,)).fetchone()
    
    if not user:
        return False, "ç”¨æˆ·ä¸å­˜åœ¨", None
    
    if bcrypt.checkpw(password.encode('utf-8'), user[1].encode('utf-8')):
        return True, "ç™»å½•æˆåŠŸ", user[0]
    else:
        return False, "å¯†ç é”™è¯¯", None

# ====================== æ•°æ®ç®¡ç†æ¨¡å— ======================
def save_user_data(user_id, df):
    conn = get_auth_db()
    try:
        # æ’å…¥ä¸»è®°å½•
        cursor = conn.cursor()
        cursor.execute('''
            INSERT INTO user_data 
            (user_id, raw_data, history_id)  -- æ˜¾å¼åŒ…å«å­—æ®µ
            VALUES (?, ?, ?)  -- æ–°å¢å ä½ç¬¦
        ''', (user_id, df.to_json(), 0))  # ç»™history_idèµ‹é»˜è®¤å€¼
        
        # è·å–æœ€æ–°data_id
        data_id = cursor.lastrowid
        
        # æ‰¹é‡æ’å…¥è¯„è®ºæ•°æ®
        comments = df[['è¯„è®º']].to_dict('records')
        cursor.executemany('''
            INSERT INTO comments (data_id, content)
            VALUES (?, ?)
        ''', [(data_id, c['è¯„è®º']) for c in comments])
        
        conn.commit()
        return True
    except Exception as e:
        conn.rollback()
        st.error(f"æ•°æ®ä¿å­˜å¤±è´¥: {str(e)}")
        return False

def load_user_data(user_id, data_type):
    """ä»æ•°æ®åº“åŠ è½½ç”¨æˆ·æ•°æ®"""
    with sqlite3.connect('user_auth.db', check_same_thread=False) as conn:
        try:
            query = f'''
                SELECT {data_type} 
                FROM user_data 
                WHERE user_id = ?
                ORDER BY data_id DESC 
                LIMIT 1
            '''
            result = conn.execute(query, (user_id,)).fetchone()
            if result and result[0]:
                return pd.read_parquet(io.BytesIO(result[0]))
            return None
        except Exception as e:
            st.error(f"æ•°æ®åŠ è½½å¤±è´¥: {str(e)}")
            return None

def save_prediction_data(user_id, df):
    """ä¿å­˜å®Œæ•´é¢„æµ‹è®°å½•ï¼ˆæ–°å¢å…³è”å­˜å‚¨ï¼‰[6,8](@ref)"""
    conn = get_auth_db()
    try:
        # åºåˆ—åŒ–é¢„æµ‹ç»“æœ
        buffer = io.BytesIO()
        df.to_parquet(buffer, index=False)
        buffer.seek(0)
        
        # æ’å…¥é¢„æµ‹è®°å½•
        cursor = conn.cursor()
        cursor.execute('''
            INSERT INTO predictions (user_id, result_data)
            VALUES (?, ?)
        ''', (user_id, buffer.getvalue()))
        
        # è·å–æœ€æ–°é¢„æµ‹ID
        prediction_id = cursor.lastrowid
        
        # æ’å…¥è¯„è®ºæ•°æ®
        comments = df[['è¯„è®º', 'ç³»ç»Ÿæ¨èæŒ‡æ•°']].to_dict('records')
        cursor.executemany('''
            INSERT INTO comments (prediction_id, content, score)
            VALUES (?, ?, ?)
        ''', [(prediction_id, c['è¯„è®º'], c['ç³»ç»Ÿæ¨èæŒ‡æ•°']) for c in comments])
        
        conn.commit()
        return True
    except Exception as e:
        conn.rollback()
        st.error(f"é¢„æµ‹æ•°æ®ä¿å­˜å¤±è´¥: {str(e)}")
        return False

@st.cache_data(ttl=3600)
def load_history_data(user_id):
    """ç¼“å­˜å†å²è®°å½•æŸ¥è¯¢[7,8](@ref)"""
    return pd.read_sql('''
        SELECT prediction_id, prediction_time, model_version 
        FROM predictions
        WHERE user_id = ?
        ORDER BY prediction_time DESC
    ''', get_auth_db(), params=(user_id,))

#-------------å†å²è®°å½•æŸ¥çœ‹-----------
def show_history(user_id):
    conn = get_auth_db()
    try:
        # è·å–ç”¨æˆ·æ‰€æœ‰é¢„æµ‹è®°å½•
        history = pd.read_sql('''
            SELECT prediction_id, prediction_time, model_version 
            FROM predictions
            WHERE user_id = ?
            ORDER BY prediction_time DESC
        ''', conn, params=(user_id,))
        
        # æ˜¾ç¤ºå†å²è®°å½•
        selected = st.selectbox("é€‰æ‹©å†å²è®°å½•", history['prediction_id'])
        
        # åŠ è½½è¯¦ç»†æ•°æ®
        detail = pd.read_sql('''
            SELECT c.content, p.result_data->>'ç³»ç»Ÿæ¨èæŒ‡æ•°' as score
            FROM predictions p
            JOIN comments c ON p.data_id = c.data_id
            WHERE p.prediction_id = ?
        ''', conn, params=(selected,))
        
        st.dataframe(detail)
    except Exception as e:
        st.error(f"å†å²è®°å½•åŠ è½½å¤±è´¥: {str(e)}")

# ====================== æ ¸å¿ƒä¸šåŠ¡æ¨¡å— ======================
def load_rebate_keywords():
    default_keywords = ['å¥½è¯„è¿”ç°', 'æ™’å›¾å¥–åŠ±', 'è¯„ä»·æœ‰ç¤¼', 'äº”æ˜Ÿå¥½è¯„', 'è¿”ç°çº¢åŒ…']
    file_path = 'rebate_keywords.txt'
    
    try:
        if os.path.exists(file_path):
            with open(file_path, 'r', encoding='utf-8') as f:
                lines = [line.strip() for line in f if line.strip()]
                valid_keywords = [kw for kw in lines if re.match(r'^[\u4e00-\u9fa5]+$', kw)]
                return valid_keywords if valid_keywords else default_keywords
        return default_keywords
    except Exception as e:
        st.error(f"âš ï¸ å…³é”®è¯æ–‡ä»¶è¯»å–å¤±è´¥ï¼Œå·²å¯ç”¨é»˜è®¤è§„åˆ™: {str(e)}")
        return default_keywords

def cleaning(df):
    progress = st.progress(0)
    status = st.status("ğŸš€ æ­£åœ¨å¤„ç†æ•°æ®...")
    
    try:
        status.write("1. è¿‡æ»¤æ±‰å­—å°‘äº5ä¸ªçš„è¯„è®º...")
        df['æ±‰å­—æ•°'] = df['è¯„è®º'].apply(lambda x: len(re.findall(r'[\u4e00-\u9fff]', str(x))))
        df = df[df['æ±‰å­—æ•°'] > 5].drop(columns=['æ±‰å­—æ•°'])
        progress.progress(16)

        status.write("2. åˆ é™¤äº§å“ä¿¡æ¯ç¼ºå¤±çš„è¯„è®º...")
        original_count = len(df)
        df = df.dropna(subset=['äº§å“'])
        removed_count = original_count - len(df)
        status.write(f"å·²æ¸…é™¤{removed_count}æ¡æ— äº§å“ä¿¡æ¯çš„è®°å½•")
        progress.progress(32)

        status.write("2.5 æ ‡å‡†åŒ–äº§å“åç§°æ ¼å¼...")
        df['äº§å“'] = df['äº§å“'].str.replace(r'[^\w\s\u4e00-\u9fa5]', '', regex=True)
        df['äº§å“'] = df['äº§å“'].str.strip().str.upper()
        progress.progress(40)

        status.write("3. æ£€æµ‹é‡å¤è¯„è®º...")
        df = df[~df.duplicated(subset=['è¯„è®º'], keep='first')]
        progress.progress(48)

        status.write("4. æ£€æµ‹å¥½è¯„è¿”ç°...")
        rebate_pattern = build_rebate_pattern()
        df = df[~df['è¯„è®º'].str.contains(rebate_pattern, na=False)]
        progress.progress(64)

        status.write("5. æ£€æµ‹å¯ç–‘æ°´å†›...")
        df = filter_spam_comments(df)
        progress.progress(80)

        df = df.reset_index(drop=True)
        progress.progress(100)
        status.update(label="âœ… æ¸…æ´—å®Œæˆï¼", state="complete")
        return df
    except Exception as e:
        status.update(label="âŒ å¤„ç†å‡ºé”™ï¼", state="error")
        st.error(f"é”™è¯¯è¯¦æƒ…ï¼š{str(e)}")
        return df

def build_rebate_pattern():
    patterns = []
    base_keywords = load_rebate_keywords()
    
    for kw in base_keywords:
        patterns.append(re.escape(kw))
        patterns.append(re.sub(r'([\u4e00-\u9fa5])', r'\1\\s*', kw))
        full_pinyin = ''.join(lazy_pinyin(kw, style=Style.NORMAL))
        patterns.append(re.sub(r'([a-z])\d?', r'\1\\d*', full_pinyin))
        initials = ''.join([p[0] for p in lazy_pinyin(kw, style=Style.INITIALS) if p])
        if initials:
            patterns.append(re.sub(r'(.)', r'\1\\W*', initials))

    base_patterns = [
        r'è¿”\s*ç°', r'è¯„.{0,3}è¿”', 
        r'åŠ \s*[å¾®Vv]', r'é¢†\s*çº¢\s*åŒ…',
        r'\d+\s*å…ƒ\s*å¥–', r'[Qqæ‰£]\\s*è£™'
    ]
    
    final_pattern = '|'.join(patterns + base_patterns)
    return re.compile(final_pattern, flags=re.IGNORECASE)

def filter_spam_comments(df):
    try:
        df['æ—¥æœŸ'] = pd.to_datetime(df['æ—¥æœŸ'])
        df_sorted = df.sort_values(['æ˜µç§°', 'åœ°åŒº', 'æ—¥æœŸ'])
        grouped = df_sorted.groupby(['æ˜µç§°', 'åœ°åŒº'])
        df_sorted['time_diff'] = grouped['æ—¥æœŸ'].diff().dt.total_seconds().abs()
        df_sorted['is_spam'] = (df_sorted['time_diff'] <= 300) & (df_sorted['time_diff'] > 0)
        return df_sorted[~df_sorted['is_spam']].drop(columns=['time_diff', 'is_spam'])
    except KeyError:
        return df

def extract_keywords(text, n=5):
    words = [word for word in jieba.cut(str(text)) if len(word) > 1]
    return ' '.join(words[:n])

def calculate_scores(row):
    try:
        text = str(row['è¯„è®º'])
        sentiment = SnowNLP(text).sentiments
        authenticity = min(len(text)/100, 1)
        relevance = len(str(row.get('å…³é”®è¯', '')).split())/10
        return pd.Series([sentiment, authenticity, relevance])
    except:
        return pd.Series([0.5, 0.5, 0.5])

def generate_analysis_prompt(product_name, comments, scores):
    return f"""è¯·æ ¹æ®ç”µå•†è¯„è®ºæ•°æ®ç”Ÿæˆäº§å“åˆ†ææŠ¥å‘Šï¼Œè¦æ±‚ï¼š
1. äº§å“åç§°ï¼š{product_name}
2. åŸºäºä»¥ä¸‹{len(comments)}æ¡çœŸå®è¯„è®ºï¼ˆè¯„åˆ†åˆ†å¸ƒï¼š{scores}ï¼‰ï¼š
{comments[:5]}...ï¼ˆæ˜¾ç¤ºå‰5æ¡ç¤ºä¾‹ï¼‰
3. è¾“å‡ºç»“æ„ï¼š
ã€äº§å“æ€»ç»“ã€‘ç”¨50å­—æ¦‚æ‹¬æ•´ä½“è¯„ä»·
ã€æ¨èæŒ‡æ•°ã€‘æ ¹æ®è¯„åˆ†åˆ†å¸ƒç»™å‡º1-10åˆ†
ã€ä¸»è¦ä¼˜ç‚¹ã€‘åˆ—å‡º3-5ä¸ªæ ¸å¿ƒä¼˜åŠ¿ï¼Œå¸¦å…·ä½“ä¾‹å­
ã€ä¸»è¦ç¼ºç‚¹ã€‘åˆ—å‡º3-5ä¸ªå…³é”®ä¸è¶³ï¼Œå¸¦å…·ä½“ä¾‹å­
ã€è´­ä¹°å»ºè®®ã€‘ç»™å‡ºæ˜¯å¦æ¨èçš„ç»“è®ºåŠåŸå› 
è¯·ç”¨markdownæ ¼å¼è¾“å‡ºï¼Œé¿å…ä¸“ä¸šæœ¯è¯­ï¼Œä¿æŒå£è¯­åŒ–"""

def call_deepseek_api(prompt):
    api_key = st.secrets["DEEPSEEK_API_KEY"]
    headers = {
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json"
    }
    data = {
        "model": "deepseek-chat",
        "messages": [{"role": "user", "content": prompt}],
        "temperature": 0.2,
        "max_tokens": 5000
    }
    
    try:
        response = requests.post("https://api.deepseek.com/v1/chat/completions", 
                                headers=headers, json=data)
        if response.status_code == 200:
            return response.json()['choices'][0]['message']['content']
        return f"APIé”™è¯¯ï¼š{response.text}"
    except Exception as e:
        return f"è¯·æ±‚å¤±è´¥ï¼š{str(e)}"

def analyze_products(df):
    analysis_results = {}
    start_time = time.time()
    
    with st.status("ğŸ” æ·±åº¦åˆ†æè¿›è¡Œä¸­...", expanded=True) as status:
        for product, group in df.groupby('äº§å“'):
            status.write(f"æ­£åœ¨åˆ†æï¼š{product}...")
            
            comments = group['è¯„è®º'].tolist()
            scores = group['ç³»ç»Ÿæ¨èæŒ‡æ•°'].value_counts().to_dict()
            
            prompt = generate_analysis_prompt(
                product_name=product,
                comments=comments,
                scores=scores
            )
            
            analysis_result = call_deepseek_api(prompt)
            analysis_results[product] = analysis_result
            
            time.sleep(0.5)
            
        duration = time.time() - start_time
        status.update(label=f"âœ… åˆ†æå®Œæˆï¼æ€»è€—æ—¶ {duration:.2f} ç§’", state="complete")
    
    return analysis_results

# ====================== ç•Œé¢æ§åˆ¶æ¨¡å— ======================
def auth_gate():
    """è®¤è¯å…¥å£é¡µé¢"""
    st.title("ç”µå•†å†³ç­–æ”¯æŒç³»ç»Ÿ")
    
    col1, col2 = st.columns(2)
    with col1:
        with st.expander("ğŸ”‘ ç”¨æˆ·ç™»å½•", expanded=True):
            login_username = st.text_input("ç”¨æˆ·å", key="login_user")
            login_password = st.text_input("å¯†ç ", type="password", key="login_pw")
            if st.button("ç™»å½•"):
                if not login_username or not login_password:
                    st.error("è¯·è¾“å…¥ç”¨æˆ·åå’Œå¯†ç ")
                else:
                    success, msg, user_id = verify_login(login_username, login_password)
                    if success:
                        st.session_state.update({
                            'logged_in': True,
                            'username': login_username,
                            'user_id': user_id,
                            'raw_df': load_user_data(user_id, 'raw_data'),
                            'cleaned_df': load_user_data(user_id, 'cleaned_data'),
                            'predicted_df': load_user_data(user_id, 'predicted_data')
                        })
                        st.rerun()
                    else:
                        st.error(msg)

    with col2:
        with st.expander("ğŸ“ æ–°ç”¨æˆ·æ³¨å†Œ", expanded=True):
            reg_username = st.text_input("æ³¨å†Œç”¨æˆ·å", key="reg_user")
            reg_password = st.text_input("æ³¨å†Œå¯†ç ", type="password", key="reg_pw")
            if st.button("ç«‹å³æ³¨å†Œ"):
                if len(reg_password) < 6:
                    st.error("å¯†ç è‡³å°‘éœ€è¦6ä½")
                elif not reg_username:
                    st.error("è¯·è¾“å…¥ç”¨æˆ·å")
                else:
                    success, msg = register_user(reg_username, reg_password)
                    if success:
                        st.success(msg + "ï¼Œè¯·è¿”å›ç™»å½•")
                    else:
                        st.error(msg)

def main_interface():
    """ä¸»ä¸šåŠ¡ç•Œé¢"""
    st.title(f"æ¬¢è¿å›æ¥ï¼Œ{st.session_state.username}ï¼")
    
    # ========= æ–°å¢ä¾§è¾¹æ å†å²è®°å½• =========
    with st.sidebar.expander("ğŸ“œ å†å²è®°å½•", expanded=True):
        try:
            history = load_history_data(st.session_state.user_id)
            
            if not history.empty:
                selected = st.selectbox(
                    "é€‰æ‹©å†å²è®°å½•", 
                    options=history['prediction_id'].astype(str) + " | " + history['prediction_time'].astype(str),
                    format_func=lambda x: f"ç‰ˆæœ¬ {x.split('|')[1].strip()}"
                )
                selected_id = int(selected.split('|')[0])
                
                # åŠ è½½è¯¦ç»†æ•°æ®
                detail = pd.read_sql('''
                    SELECT c.content, c.score 
                    FROM predictions p
                    JOIN comments c ON p.prediction_id = c.prediction_id
                    WHERE p.prediction_id = ?
                ''', get_auth_db(), params=(selected_id,))
                
                st.dataframe(detail, 
                           column_config={
                               "content": "è¯„è®ºå†…å®¹",
                               "score": st.column_config.ProgressColumn(
                                   "æ¨èåº¦",
                                   min_value=1,
                                   max_value=10
                               )
                           },
                           height=200)
            else:
                st.info("æš‚æ— å†å²è®°å½•")
        except Exception as e:
            st.error(f"å†å²è®°å½•åŠ è½½å¤±è´¥: {str(e)}")    
    
    # æ–‡ä»¶ä¸Šä¼ æ¨¡å—
    uploaded_file = st.file_uploader("ä¸Šä¼ CSVæ–‡ä»¶", type=["csv"], 
                                    help="æ”¯æŒUTF-8ç¼–ç æ–‡ä»¶ï¼Œæœ€å¤§100MB")
    
    if uploaded_file:
        try:
            raw_df = pd.read_csv(uploaded_file)
            conn = get_auth_db()
            conn.execute('''
                INSERT INTO user_data (user_id, raw_data)
                VALUES (?, ?)
            ''', (st.session_state.user_id, uploaded_file.getvalue()))
            conn.commit()
            st.session_state.raw_df = raw_df
        except Exception as e:
            st.error(f"æ–‡ä»¶è¯»å–å¤±è´¥: {str(e)}")

    # æ•°æ®å±•ç¤ºæ¨¡å—
    if st.session_state.raw_df is not None:
        with st.expander("ğŸ“‚ åŸå§‹æ•°æ®è¯¦æƒ…", expanded=False):
            st.write(f"è®°å½•æ•°ï¼š{len(st.session_state.raw_df)}")
            st.dataframe(st.session_state.raw_df, use_container_width=True, height=300)
            if st.button("ğŸ—‘ï¸ æ¸…é™¤å½“å‰æ•°æ®"):
                st.session_state.raw_df = None
                st.session_state.cleaned_df = None
                st.session_state.predicted_df = None
                st.rerun()

    # æ•°æ®æ¸…æ´—æ¨¡å—
    if st.session_state.raw_df is not None:
        st.divider()
        st.subheader("æ•°æ®æ¸…æ´—æ¨¡å—")
        
        if st.button("ğŸš€ å¼€å§‹æ¸…æ´—", help="ç‚¹å‡»å¼€å§‹ç‹¬ç«‹æ¸…æ´—æµç¨‹", use_container_width=True):
            with st.spinner('æ­£åœ¨å¤„ç†æ•°æ®...'):
                start_time = time.time()
                cleaned_df = cleaning(st.session_state.raw_df.copy())
                if save_user_data(st.session_state.user_id, 'cleaned_data', cleaned_df):
                    st.session_state.cleaned_df = cleaned_df
                st.session_state.processing_time = time.time() - start_time

        if st.session_state.cleaned_df is not None:
            with st.expander("âœ¨ æ¸…æ´—åæ•°æ®è¯¦æƒ…", expanded=False):
                st.write(f"å”¯ä¸€äº§å“åˆ—è¡¨ï¼š{st.session_state.cleaned_df['äº§å“'].unique().tolist()}")
                st.dataframe(
                    st.session_state.cleaned_df[['æ˜µç§°','æ—¥æœŸ','åœ°åŒº','äº§å“', 'è¯„åˆ†','è¯„è®º']],
                    use_container_width=True,
                    height=400
                )

    # ====================== é¢„æµ‹åˆ†ææ¨¡å— ======================
    if st.session_state.cleaned_df is not None:
        st.divider()
        st.subheader("é¢„æµ‹æ¨¡å—")
    
        if st.button("ğŸ”® é¢„æµ‹æ¨èæŒ‡æ•°", use_container_width=True):
            if 'model' not in st.session_state:
                st.error("æ¨¡å‹æœªåŠ è½½ï¼Œæ— æ³•è¿›è¡Œé¢„æµ‹ï¼")
                return
        
            cleaned_df = st.session_state.cleaned_df.copy()
        
            with st.status("ğŸ§  æ­£åœ¨ç”Ÿæˆé¢„æµ‹...", expanded=True) as status:
                try:
                # ç‰¹å¾æå–æµç¨‹
                    status.write("1. æå–å…³é”®è¯...")
                    cleaned_df['å…³é”®è¯'] = cleaned_df['è¯„è®º'].apply(lambda x: extract_keywords(x, n=5))
                
                    status.write("2. è®¡ç®—æƒ…æ„Ÿç‰¹å¾...")
                    scores = cleaned_df.apply(calculate_scores, axis=1)
                    cleaned_df[['æƒ…æ„Ÿåº¦', 'çœŸå®æ€§', 'å‚è€ƒåº¦']] = scores
                
                    status.write("3. æ–‡æœ¬ç‰¹å¾è½¬æ¢...")
                    keywords_tfidf = st.session_state.tfidf.transform(cleaned_df['å…³é”®è¯'])
                
                    status.write("4. åˆå¹¶ç‰¹å¾...")
                    numeric_features = cleaned_df[['æƒ…æ„Ÿåº¦', 'çœŸå®æ€§', 'å‚è€ƒåº¦']].values
                    features = hstack([keywords_tfidf, numeric_features])
                
                    status.write("5. å¤„ç†åˆ†ç±»ç‰¹å¾...")
                    cleaned_df['åœ°åŒº_ç¼–ç '] = pd.Categorical(
                        cleaned_df['åœ°åŒº'], 
                        categories=st.session_state.region_mapping
                    ).codes
                    cleaned_df['äº§å“_ç¼–ç '] = pd.Categorical(
                        cleaned_df['äº§å“'],
                        categories=st.session_state.product_mapping
                    ).codes
                    final_features = hstack([features, cleaned_df[['åœ°åŒº_ç¼–ç ', 'äº§å“_ç¼–ç ']].values])
                
                    status.write("6. è¿›è¡Œæ¨¡å‹é¢„æµ‹...")
                    predicted_scores = st.session_state.model.predict(final_features)
                    cleaned_df['ç³»ç»Ÿæ¨èæŒ‡æ•°'] = np.round(predicted_scores).clip(1, 10).astype(int)
                
                    if save_prediction_data(st.session_state.user_id, cleaned_df):
                        st.session_state.predicted_df = cleaned_df[['äº§å“', 'è¯„è®º', 'ç³»ç»Ÿæ¨èæŒ‡æ•°']]
                    
                except Exception as e:
                    status.update(label="âŒ é¢„æµ‹å‡ºé”™ï¼", state="error")
                    st.error(f"é”™è¯¯è¯¦æƒ…ï¼š{str(e)}")
                    st.stop()

            # é¢„æµ‹ç»“æœå±•ç¤º
            if st.session_state.predicted_df is not None:
                st.success("é¢„æµ‹ç»“æœï¼š")
                st.dataframe(
                    st.session_state.predicted_df,
                    use_container_width=True,
                    height=600,
                    hide_index=True,
                    column_config={
                        "äº§å“": "å•†å“åç§°",
                        "è¯„è®º": st.column_config.TextColumn(width="large"),
                        "ç³»ç»Ÿæ¨èæŒ‡æ•°": st.column_config.ProgressColumn(
                            "æ¨èåº¦",
                            help="AIæ¨èæŒ‡æ•°(1-10)",
                            format="%d",
                            min_value=1,
                            max_value=10
                        )
                    }
            )
            st.caption(f"æ€»è®°å½•æ•°ï¼š{len(st.session_state.predicted_df)} æ¡")
    
            # æ•°æ®å¯¼å‡ºåŠŸèƒ½
            csv = st.session_state.predicted_df.to_csv(index=False).encode('utf-8')
            st.download_button(
                label="â¬‡ï¸ ä¸‹è½½é¢„æµ‹ç»“æœ",
                data=csv,
                file_name='predicted_scores.csv',
                mime='text/csv',
                key='prediction_download'
            )

    # ====================== æ·±åº¦åˆ†ææ¨¡å— ======================
    if st.session_state.predicted_df is not None:
        st.divider()
        st.subheader("æ·±åº¦åˆ†ææ¨¡å—")
    
        if st.button("ğŸ“Š ç”Ÿæˆäº§å“åˆ†ææŠ¥å‘Š", type="primary"):
            analysis_results = analyze_products(st.session_state.predicted_df)
            st.session_state.analysis_reports = analysis_results
        
            # æŠ¥å‘Šå±•ç¤ºç»„ä»¶
            for product, report in analysis_results.items():
                with st.expander(f"â€‹**â€‹{product}â€‹**â€‹ å®Œæ•´åˆ†ææŠ¥å‘Š", expanded=False):
                    st.markdown(report)

            # æ‰¹é‡å¯¼å‡ºåŠŸèƒ½
            if analysis_results:
                zip_buffer = io.BytesIO()
                with zipfile.ZipFile(zip_buffer, 'w') as zip_file:
                    for product, report in analysis_results.items():
                        safe_name = re.sub(r'[\\/*?:"<>|]', "_", product)
                        zip_file.writestr(f"{safe_name}_åˆ†æ.txt", report)
                zip_buffer.seek(0)
            
                st.download_button(
                    label="â¬‡ï¸ ä¸‹è½½å…¨éƒ¨åˆ†ææŠ¥å‘Š",
                    data=zip_buffer,
                    file_name="äº§å“åˆ†ææŠ¥å‘Š.zip",
                    mime="application/zip",
                    key='full_report_download'
                )

# ====================== ä¸»ç¨‹åºå…¥å£ ======================
if __name__ == "__main__":
    # åˆå§‹åŒ–æ¨¡å‹å’Œæ•°æ®åº“
    if 'model' not in st.session_state:
        try:
            # åŠ è½½æœºå™¨å­¦ä¹ æ¨¡å‹
            st.session_state.model = joblib.load('model.joblib')
            st.session_state.tfidf = joblib.load('tfidf_vectorizer.joblib')
            category_mappings = joblib.load('category_mappings.joblib')
            st.session_state.region_mapping = category_mappings['region']
            st.session_state.product_mapping = category_mappings['product']
        except Exception as e:
            st.error(f"åˆå§‹åŒ–å¤±è´¥: {str(e)}")

    # åˆå§‹åŒ–ä¼šè¯çŠ¶æ€
    session_keys = ['logged_in', 'username', 'user_id', 'raw_df', 'cleaned_df', 'predicted_df']
    for key in session_keys:
        if key not in st.session_state:
            st.session_state[key] = None

    # é¡µé¢é…ç½®
    st.set_page_config(
        page_title="ç”µå•†ç”¨æˆ·è´­ä¹°å†³ç­–AIè¾…åŠ©æ”¯æŒç³»ç»Ÿ",
        layout="wide",
        page_icon="ğŸ›’",
        initial_sidebar_state="expanded"
    )
    
    # æµç¨‹æ§åˆ¶
    if not st.session_state.logged_in:
        auth_gate()
    else:
        main_interface()