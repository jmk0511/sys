import time  
import streamlit as st
import pandas as pd
import re
from pypinyin import lazy_pinyin, Style
from datetime import datetime
import jieba

from sklearn.feature_extraction.text import TfidfVectorizer
from scipy.sparse import hstack
import joblib

# åˆå§‹åŒ–sessionçŠ¶æ€ï¼ˆå…³é”®æ•°æ®ç»“æ„éš”ç¦»ï¼‰
if 'raw_df' not in st.session_state: # åŸå§‹æ•°æ®
    st.session_state.raw_df = None
if 'cleaned_df' not in st.session_state: # æ¸…æ´—åæ•°æ®
    st.session_state.cleaned_df = None
if 'predicted_df' not in st.session_state: # é¢„æµ‹ç»“æœ
    st.session_state.predicted_df = None

# åŠ è½½æ¨¡å‹å’Œé¢„å¤„ç†å¯¹è±¡
if 'model' not in st.session_state:
    try:
        # åŠ è½½æ¨¡å‹
        st.session_state.model = joblib.load('model.joblib')
        # åŠ è½½TF-IDFå‘é‡å™¨
        st.session_state.tfidf = joblib.load('tfidf_vectorizer.joblib')
        # åŠ è½½åˆ†ç±»æ˜ å°„
        category_mappings = joblib.load('category_mappings.joblib')
        st.session_state.region_mapping = category_mappings['region']
        st.session_state.product_mapping = category_mappings['product']
    except Exception as e:
        st.error(f"åˆå§‹åŒ–å¤±è´¥: {str(e)}")
        
        
# é¡µé¢é…ç½®
st.set_page_config(page_title="CSVæ•°æ®æ¸…æ´—å·¥å…·", layout="wide")
st.title("è‡ªåŠ¨åŒ–æ•°æ®æ¸…æ´—å·¥å…·")

def cleaning(df):
    """æ ¸å¿ƒæ¸…æ´—é€»è¾‘ï¼ˆå‚è€ƒç½‘é¡µ5ã€ç½‘é¡µ6çš„æ¸…æ´—æµç¨‹ï¼‰"""
    progress = st.progress(0)
    status = st.status("ğŸš€ æ­£åœ¨å¤„ç†æ•°æ®...")
    
    try:
        # æ­¥éª¤1ï¼šåŸºç¡€è¿‡æ»¤
        status.write("1. è¿‡æ»¤æ±‰å­—å°‘äº5ä¸ªçš„è¯„è®º...")
        # è®¡ç®—æ¯ä¸ªè¯„è®ºä¸­çš„æ±‰å­—æ•°é‡
        df['æ±‰å­—æ•°'] = df['è¯„è®º'].apply(lambda x: len(re.findall(r'[\u4e00-\u9fff]', str(x))))
        # ä¿ç•™æ±‰å­—æ•°è¶…è¿‡5çš„è¡Œ
        df = df[df['æ±‰å­—æ•°'] > 5].drop(columns=['æ±‰å­—æ•°'])  # 
        progress.progress(20)

        # æ­¥éª¤2ï¼šé‡å¤è¯„è®ºè¿‡æ»¤ï¼ˆå‚è€ƒç½‘é¡µ6é‡å¤å€¼å¤„ç†ï¼‰
        status.write("2. æ£€æµ‹é‡å¤è¯„è®º...")
        df = df[~df.duplicated(subset=['è¯„è®º'], keep='first')]
        progress.progress(40)

        # æ­¥éª¤3ï¼šè¿”ç°æ£€æµ‹ï¼ˆå‚è€ƒç½‘é¡µ1æ–‡æœ¬æ¸…æ´—ï¼‰
        status.write("3. æ£€æµ‹å¥½è¯„è¿”ç°...")
        rebate_pattern = build_rebate_pattern()
        df = df[~df['è¯„è®º'].str.contains(rebate_pattern, na=False)]
        progress.progress(60)

        # æ­¥éª¤4ï¼šæ°´å†›æ£€æµ‹ï¼ˆå‚è€ƒç½‘é¡µ8æ—¶é—´åºåˆ—å¤„ç†ï¼‰
        status.write("4. æ£€æµ‹å¯ç–‘æ°´å†›...")
        df = filter_spam_comments(df)
        progress.progress(80)

        # æœ€ç»ˆå¤„ç†
        df = df.reset_index(drop=True)
        progress.progress(100)
        status.update(label="âœ… æ¸…æ´—å®Œæˆï¼", state="complete")
        return df
    except Exception as e:
        status.update(label="âŒ å¤„ç†å‡ºé”™ï¼", state="error")
        st.error(f"é”™è¯¯è¯¦æƒ…ï¼š{str(e)}")
        return df

def build_rebate_pattern():
    """æ„å»ºè¿”ç°æ£€æµ‹æ­£åˆ™ï¼ˆå‚è€ƒç½‘é¡µ3æ–‡æœ¬å¤„ç†æŠ€å·§ï¼‰"""
    base_keywords = ['å¥½è¯„è¿”ç°', 'æ™’å›¾å¥–åŠ±', 'è¯„ä»·æœ‰ç¤¼']
    patterns = []
    
    # ç”Ÿæˆæ‹¼éŸ³å˜ä½“ï¼ˆå‚è€ƒç½‘é¡µ1æ–‡æœ¬æ¸…æ´—ï¼‰
    for kw in base_keywords:
        full_pinyin = ''.join(lazy_pinyin(kw, style=Style.NORMAL))
        patterns.append(re.escape(full_pinyin))
        
        initials = ''.join([p[0] for p in lazy_pinyin(kw, style=Style.INITIALS) if p])
        if initials:
            patterns.append(re.escape(initials))
    
    # é€šç”¨æ¨¡å¼ï¼ˆå‚è€ƒç½‘é¡µ6å¼‚å¸¸å€¼å¤„ç†ï¼‰
    patterns += [
        r'è¿”\s*ç°', r'è¯„.{0,3}è¿”', 
        r'åŠ \s*å¾®', r'é¢†\s*çº¢\s*åŒ…',
        r'\d+\s*å…ƒ\s*å¥–'
    ]
    return re.compile('|'.join(patterns), flags=re.IGNORECASE)

def filter_spam_comments(df):
    """æ°´å†›æ£€æµ‹ç®—æ³•ï¼ˆå‚è€ƒç½‘é¡µ8æ—¶é—´åºåˆ—å¤„ç†ï¼‰"""
    try:
        # è½¬æ¢æ—¶é—´æ ¼å¼
        df['æ—¥æœŸ'] = pd.to_datetime(df['æ—¥æœŸ'])
        
        # æŒ‰æ˜µç§°å’Œåœ°åŒºåˆ†ç»„æ’åº
        df_sorted = df.sort_values(['æ˜µç§°', 'åœ°åŒº', 'æ—¥æœŸ'])
        grouped = df_sorted.groupby(['æ˜µç§°', 'åœ°åŒº'])
        
        # è®¡ç®—æ—¶é—´å·®ï¼ˆå‚è€ƒç½‘é¡µ5æ•°æ®èŒƒå›´çº¦æŸï¼‰
        df_sorted['time_diff'] = grouped['æ—¥æœŸ'].diff().dt.total_seconds().abs()
        
        # æ ‡è®°å¯ç–‘è®°å½•ï¼ˆ5åˆ†é’Ÿå†…å¤šæ¬¡è¯„ä»·ï¼‰
        df_sorted['is_spam'] = (df_sorted['time_diff'] <= 300) & (df_sorted['time_diff'] > 0)
        
        return df_sorted[~df_sorted['is_spam']].drop(columns=['time_diff', 'is_spam'])
    except KeyError:
        return df


def predict_recommend(df):
    """æ‰§è¡Œæ¨èæŒ‡æ•°é¢„æµ‹ï¼ˆé›†æˆå®Œæ•´ç‰¹å¾å·¥ç¨‹ï¼‰"""
    if 'model' not in st.session_state:
        st.error("æ¨¡å‹æœªåŠ è½½")
        return df

    with st.status("ğŸ”® æ­£åœ¨ç”Ÿæˆé¢„æµ‹..."):
        try:
            # å¤åˆ¶æ•°æ®é¿å…æ±¡æŸ“åŸæ•°æ®
            df = df.copy()
            
            # 0. å¤‡ä»½åŸå§‹äº§å“åç§°ï¼ˆå…³é”®æ­¥éª¤ï¼‰
            original_product = df['äº§å“'].copy()
            
            # 1. æå–å…³é”®è¯ï¼ˆä¸è®­ç»ƒæ—¶å®Œå…¨ä¸€è‡´ï¼‰
            st.write("1. æå–å…³é”®è¯...")
            def extract_keywords(text):
                words = [word for word in jieba.cut(str(text)) if len(word) > 1]
                return ' '.join(words[:5])  # ä¿æŒè®­ç»ƒæ—¶çš„n=5
            df['å…³é”®è¯'] = df['è¯„è®º'].apply(extract_keywords)

            # 2. è®¡ç®—æ•°å€¼ç‰¹å¾ï¼ˆå®Œå…¨å¤åˆ¶è®­ç»ƒé€»è¾‘ï¼‰
            st.write("2. è®¡ç®—æƒ…æ„Ÿç‰¹å¾...")
            def calculate_features(row):
                try:
                    sentiment = SnowNLP(row['è¯„è®º']).sentiments
                    authenticity = min(len(str(row['è¯„è®º']))/100, 1)
                    relevance = len(row['å…³é”®è¯'].split())/10
                    return pd.Series([sentiment, authenticity, relevance])
                except:
                    return pd.Series([0.5, 0.5, 0.5])  # å¼‚å¸¸å¤„ç†ä¿æŒä¸€è‡´
            df[['æƒ…æ„Ÿåº¦', 'çœŸå®æ€§', 'å‚è€ƒåº¦']] = df.apply(calculate_features, axis=1)

            # 3. å¤„ç†åˆ†ç±»ç‰¹å¾ç¼–ç ï¼ˆåˆ›å»ºæ–°åˆ—ï¼‰
            st.write("3. ç¼–ç åˆ†ç±»ç‰¹å¾...")
            df['åœ°åŒº_ç¼–ç '] = pd.Categorical(
                df['åœ°åŒº'], 
                categories=st.session_state.region_mapping
            ).codes.replace(-1, 0)  # å¤„ç†æœªçŸ¥ç±»åˆ«
            
            df['äº§å“_ç¼–ç '] = pd.Categorical(
                df['äº§å“'], 
                categories=st.session_state.product_mapping
            ).codes.replace(-1, 0)

            # 4. ç”ŸæˆTF-IDFç‰¹å¾ï¼ˆä½¿ç”¨è®­ç»ƒæ—¶çš„vectorizerï¼‰
            st.write("4. ç”Ÿæˆæ–‡æœ¬ç‰¹å¾...")
            tfidf_features = st.session_state.tfidf.transform(df['å…³é”®è¯'])

            # 5. æ„å»ºç‰¹å¾çŸ©é˜µï¼ˆä¿æŒè®­ç»ƒæ—¶ç»“æ„ï¼‰
            st.write("5. ç»„åˆç‰¹å¾...")
            num_features = ['æƒ…æ„Ÿåº¦', 'çœŸå®æ€§', 'å‚è€ƒåº¦']
            categorical_features = ['åœ°åŒº_ç¼–ç ', 'äº§å“_ç¼–ç ']
            
            # å®Œå…¨å¤åˆ¶è®­ç»ƒæ—¶çš„hstackç»“æ„
            features = hstack([
                tfidf_features,
                df[num_features].values.astype(float),
                df[categorical_features].values.astype(int)
            ])

            # 6. æ‰§è¡Œé¢„æµ‹ï¼ˆä¿æŒè®­ç»ƒæ—¶åå¤„ç†ï¼‰
            st.write("6. è¿›è¡Œé¢„æµ‹...")
            predictions = st.session_state.model.predict(features)
            df['æ¨èæŒ‡æ•°'] = predictions.round().clip(1, 10).astype(int)
            
            # 7. æ¢å¤åŸå§‹äº§å“åç§°ï¼ˆå…³é”®æ­¥éª¤ï¼‰
            df['äº§å“'] = original_product
            
            return df[['äº§å“', 'è¯„è®º', 'æ¨èæŒ‡æ•°']]  # ç¡®ä¿è¾“å‡ºåŸå§‹åç§°

        except Exception as e:
            st.error(f"é¢„æµ‹å¤±è´¥: {str(e)}")
            import traceback
            st.error(traceback.format_exc())
            return df

# æ–‡ä»¶ä¸Šä¼ æ¨¡å—ï¼ˆå§‹ç»ˆæ˜¾ç¤ºï¼‰
uploaded_file = st.file_uploader("ä¸Šä¼ CSVæ–‡ä»¶", type=["csv"], 
                               help="æ”¯æŒUTF-8ç¼–ç æ–‡ä»¶ï¼Œæœ€å¤§100MB")

if uploaded_file and st.session_state.raw_df is None:
    st.session_state.raw_df = pd.read_csv(uploaded_file)

# æ˜¾ç¤ºåŸå§‹æ•°æ®ï¼ˆä¸Šä¼ åæ°¸ä¹…å¯æŸ¥çœ‹ï¼‰
if st.session_state.raw_df is not None:
    with st.expander("ğŸ“‚ æ°¸ä¹…æŸ¥çœ‹åŸå§‹æ•°æ®", expanded=True):
        st.write(f"åŸå§‹è®°å½•æ•°ï¼š{len(st.session_state.raw_df)}")
        st.dataframe(st.session_state.raw_df.head(3), use_container_width=True)
        
   
# æ¸…æ´—æ¨¡å—ï¼ˆç‹¬ç«‹æŒ‰é’®ç»„ï¼‰
if st.session_state.raw_df is not None:
    st.divider()
    st.subheader("æ•°æ®æ¸…æ´—æ¨¡å—")
    
    col1, col2 = st.columns([1,3])
    with col1:
        # è§¦å‘æ¸…æ´—
        if st.button("ğŸš€ å¼€å§‹æ¸…æ´—", help="ç‚¹å‡»å¼€å§‹ç‹¬ç«‹æ¸…æ´—æµç¨‹", 
                   use_container_width=True):
            with st.spinner('æ­£åœ¨å¤„ç†æ•°æ®...'):
                start_time = time.time()
                st.session_state.cleaned_df = cleaning(st.session_state.raw_df.copy())
                st.session_state.processing_time = time.time() - start_time

    # æ˜¾ç¤ºæ¸…æ´—ç»“æœ                
    if st.session_state.cleaned_df is not None:
        with col2:
            if st.button("ğŸ” æŸ¥çœ‹æ¸…æ´—ç»“æœ", help="ç‹¬ç«‹æŸ¥çœ‹æ¸…æ´—æ•°æ®", 
                       use_container_width=True):
                with st.expander("âœ¨ æ¸…æ´—åæ•°æ®è¯¦æƒ…", expanded=True):
                    st.dataframe(
                        st.session_state.cleaned_df[['äº§å“', 'è¯„è®º']],
                        use_container_width=True,
                        height=400
                    )
                    # ä¸‹è½½æ¸…æ´—ç»“æœ
                    csv = st.session_state.cleaned_df.to_csv(index=False).encode('utf-8')
                    st.download_button(
                        label="â¬‡ï¸ ä¸‹è½½æ¸…æ´—æ•°æ®",
                        data=csv,
                        file_name='cleaned_data.csv',
                        mime='text/csv'
                    )
                    
                    
# é¢„æµ‹æ¨¡å—ï¼ˆç‹¬ç«‹åŠŸèƒ½ï¼‰
if st.session_state.cleaned_df is not None:
    st.divider()
    st.subheader("æ¨èæŒ‡æ•°é¢„æµ‹æ¨¡å—")
    
    col_pred1, col_pred2 = st.columns([1,3])
    with col_pred1:
        # è§¦å‘é¢„æµ‹
        if st.button("âœ¨ ç”Ÿæˆæ¨èæŒ‡æ•°", type="primary", 
                   help="åŸºäºæ¸…æ´—æ•°æ®ç‹¬ç«‹é¢„æµ‹", use_container_width=True):
            with st.spinner('é¢„æµ‹è¿›è¡Œä¸­...'):
                start_pred = time.time()
                try:
                    # æ‰§è¡Œé¢„æµ‹å¹¶ä¿ç•™åŸå§‹åˆ—
                    predicted_df = predict_recommend(st.session_state.cleaned_df.copy())
                    # ç²¾ç¡®ç­›é€‰ç›®æ ‡å­—æ®µï¼ˆç¡®ä¿åˆ—å­˜åœ¨æ€§éªŒè¯ï¼‰
                    required_columns = ['äº§å“', 'è¯„è®º', 'æ¨èæŒ‡æ•°']
                    if all(col in predicted_df.columns for col in required_columns):
                        st.session_state.predicted_df = predicted_df[required_columns]
                        st.session_state.pred_time = time.time() - start_pred
                        st.toast(f"é¢„æµ‹å®Œæˆï¼è€—æ—¶{st.session_state.pred_time:.2f}ç§’", icon="âœ…")
                    else:
                        st.error("é¢„æµ‹ç»“æœç¼ºå°‘å¿…è¦å­—æ®µ")
                except Exception as e:
                    st.error(f"é¢„æµ‹å¼‚å¸¸: {str(e)}")

    # æ˜¾ç¤ºé¢„æµ‹ç»“æœ            
    if st.session_state.predicted_df is not None:
        with col_pred2:
            if st.button("ğŸ” æŸ¥çœ‹é¢„æµ‹ç»“æœ", type="primary", 
                    help="ç‹¬ç«‹æŸ¥çœ‹é¢„æµ‹æ•°æ®", use_container_width=True):
                with st.expander("ğŸ“ˆ é¢„æµ‹ç»“æœè¯¦æƒ…", expanded=True):
                    # ç›´æ¥æ˜¾ç¤ºåŸå§‹æ•°å€¼ï¼ˆç§»é™¤æ ¼å¼è½¬æ¢ï¼‰
                    display_df = st.session_state.predicted_df.copy()
                
                    # ä½¿ç”¨åŸç”Ÿdataframeæ˜¾ç¤º
                    st.dataframe(
                        display_df,
                        use_container_width=True,
                        height=400
                    )
                
                    # ä¸‹è½½åŠŸèƒ½ä¿æŒä¸å˜
                    csv = st.session_state.predicted_df.to_csv(index=False).encode('utf-8')
                    st.download_button(
                        label="â¬‡ï¸ ä¸‹è½½é¢„æµ‹æ•°æ®",
                        data=csv,
                        file_name='predicted_data.csv',
                        mime='text/csv',
                        help="ä¸‹è½½åŒ…å«1-10åˆ†åŸå§‹è¯„åˆ†çš„æ•°æ®æ–‡ä»¶"
                    )    
        

